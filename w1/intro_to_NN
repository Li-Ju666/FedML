#### By Li Ju ####
#### 2020/22/06 ####

This is my study note for neural network. In this article, the most basic model, 
fully-connected neural network will be introduced. 

Neural network is inspired by biological neural network, which is consisted of numerous connected neurons. 
Each neuron has multiple input and output, and is basically an operator, 
accepting input current and producing output current to its target neurons. 
Artificial neural network is a simulation for this, but of course, a lot of modification can be done. 

[What is node]
A node is a structure with weights for every input, a bias term and a function, named activation function. 
The node sums up all its input values with weights and bias, 
calculates its output value with activation function. 
Then, the output value will be input to other nodes, which are assumed as "connect with this node". 
Normally most activation functions are not linear, like sigmoid, Relu and others. 

Mathematically, if we define input values as vector A, weights as W, bias term as b, activation
function as F_activate, and output value as z, the function of a node is much simpler: 
z = F_activate(W*A+b) and z is the input of next node. 


[What is layer]
To organize nodes in a better way, the concept of layers is introduced. 
In fully-connected network model, nodes within a single layer are not connected, 
instead, they connect with every nodes at its next layer. 
Networks with more than 3 layers(1 input layer, 1 output layer and 1 hidden layer) are generally
called deep neural network. 

[Dimensionality]
As all the computation will be done with matrix operation, it is important to know how each part of 
network is represented. Basically there are two approaches to represent input dataset, feature-as-row 
and sample-as-row. 

DEFINE: n - number of samples; M - number of features
	m_i - number of nodes in the ith layer; 
	F_af = activation function; 
	M(i*j) - M is a matrix of size i(rows)*j(columns); 
	Z - output matrix of a layer; A - input matrix of a layer; B - bias term matrix of a layer; 
	X - original dataset
For each layer, the computation can be represented as following: 
1). if original dataset is of feature-as-row: X(M * n)
	Z(m_i * n) = F_af(W(m_i * m_{i-1}) * A(m_{i-1}*n) + B(m_i*1))
2). if original dataset is of sample-as-row: X(n*M)
	Z(n*m_i) = F_af(A(n * m_{i-1}) * W(m_{i-1} * m_i) + B(1*m_i))

One thing is trivial that: the dimensionality of W*A (or A*W) is not the same as matrix B: here matrix B
will extend itself as a m_i * n matrix repeating its own values and be added to WA matrix element-wise. 

[Forward propagation]
Forward propagaion is simplly do operations on input data layer by layer: 

DEFINE: X - original dataset; F_af_i - activation function of layer i; W_i - weight matrix of layer i; 
	B_i - bias term vector of layer i; 

Then forward propagation is mathematically for a 3 layer neural network: 
	Output = F_af_3((W_3(F_af_2(W_2 * (F_af_1(W_1 * X + B_1)) + B_2)) + B_3). 

[Cost function]
An output matrix can be got with an input, however, it is required to estimate how biased of our model is. 
Therefore, cost functions are defined to quantify difference between as-obtained output and target output. 
Generally two different cost functions are commonly used: squared error function and cross entropy function. 

Squared error function: E = sum 1/2(target-output)^2
Cross entropy function: E = sum target*log(output)

[How to learn]
With cost function, it is clear that how biased our model is now. However, how to improve the model? 
Here are two different approaches: 
1. modify weights of different input nodes for each node; 
2. modify activation function of each node, which is more complicated. 

Therefore generally weights are modified based on cost function to achieve better performance. 
Here back propagation is requried to update weights. 

[Back propagation]
In computation from input to output, input dataset are transformed layer by layer forward and finally
output data are obtained. To update weights of each node, error between output will be tranmistted backward
layer by layer to each node. 

DEFINE: W_i - weights of layer i; B_i - bias term of layer i; 
	A_i - input data of layer i; Z_i - weighted input data of layer i: Z_i = W_i + B_i; 
	F_i - activation function of layer_i; d(y)/d(x): differentiate y with resoect to x; 
	X - input dataset; Y - target output; E - Error represent with cost function C; 

Here we will discuss about a 3-layer neural network. 
    A_1 = Z_1 = A_2 = X; 
    Z_2 = W_2 * A_2 + B_2; 
    A_3 = F_2(Z_2); 
    Z_3 = W_3 * A_3 + B_3; 
    output = F_3(Z_3); 
    E = C(output - Y); 

Before further discussion, firstly delta_i will be defined as the derivation of E with respect to 
weighted input of layer i: d(E)/d(Z_i), which stands for error (vector) of layer i. 
For the last layer - layer L, delta is: 
	delta_L = d(E)/d(Z_L) = d(E)/d(output) * d(output)/d(Z_L)
		= C'(output) * F_L'(Z_L)	    (1)
For a random neural network, delta of the ith layer is: 
	delta_i = delta_{i+1} * W_{i+1} * F_i'(Z_i)	(2)

Taking advantages of delta, we can easily get the derivation of E with respect to W_i and B_i: 
	d(E)/d(W_i) = d(E)/d(Z_i) * d(Z_i)/d(W_i) = delta_i * A_i	(3)
	d(E)/d(B_i) = d(E)/d(Z_i) * d(Z_i)/d(B_i) = delta_i;		(4)

With equation (1, 2, 3, 4), one can easily how adjust on each weight/bias will influence cost function. 

To adjust paramters, learning rate n is defined and the parameters are modified with folloing equation: 
	p -= n * d(E)/d(p)

This is called gradient descent. 


